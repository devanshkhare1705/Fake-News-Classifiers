{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_rEtghp-BkC"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHuRe8943V84"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy, matplotlib.pyplot as plt, scipy\n",
    "from scipy.stats import entropy\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKM4pkME-L7u"
   },
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXAnTWW53rQh"
   },
   "outputs": [],
   "source": [
    "def load_data(clean_real_text,clean_fake_text):\n",
    "    combined_text = []\n",
    "    labels = []\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    with open (clean_real_text,'r') as real:\n",
    "        lines = real.readlines()  \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            combined_text.append(line)\n",
    "            labels.append(1)\n",
    "\n",
    "    with open (clean_fake_text) as fake:\n",
    "        lines = fake.readlines()   \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            combined_text.append(line)\n",
    "            labels.append(0)\n",
    "\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(combined_text, labels, test_size = 0.15,train_size = 0.85) #X and Y have to be the same size\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = 0.1765, train_size = 0.8235)\n",
    "\n",
    "    vector_training = cv.fit_transform(X_train).toarray()\n",
    "    vector_validation = cv.transform(X_validation)\n",
    "    vector_test = cv.transform(X_test)\n",
    "    feature_names = cv.get_feature_names_out()\n",
    "    feature_names = feature_names.tolist()\n",
    "    outputs = [vector_training, Y_train, vector_validation, Y_validation, vector_test, Y_test, feature_names]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwYwYmEpCGIU"
   },
   "source": [
    "Optimal Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtc9vYGT3rgP"
   },
   "outputs": [],
   "source": [
    "def select_model(data, max_depths, split_criteria):\n",
    "    max_depth_accuracies = []; model_characteristics = []; tree_models = [] #model_characteristics = [[\"max_depth\",\"split\",\"accuracy\",\"model\"]]\n",
    "\n",
    "    for criteria in split_criteria:\n",
    "        for depth in max_depths:\n",
    "\n",
    "            #train decision tree\n",
    "            model = DecisionTreeClassifier(criterion = criteria,max_depth = depth)  #initialize decision tree\n",
    "            model.fit(data[0],data[1])   #train decision tree\n",
    "\n",
    "            #validation accuracy assessment\n",
    "            predictions = model.predict(data[2])\n",
    "            model_accuracy = accuracy_score(data[3],predictions)\n",
    "\n",
    "            #recording validation accuracies\n",
    "            max_depth_accuracies.append(model_accuracy)   #set of all validation accuracies\n",
    "            tree_models.append(model)\n",
    "            model_characteristics.append([depth,criteria,model_accuracy,model])   #set of characteristics of all trees\n",
    "\n",
    "\n",
    "    max_accuracy_index = max_depth_accuracies.index(max(max_depth_accuracies))  #find index of highest accuracy\n",
    "    selected_model = tree_models[max_accuracy_index]\n",
    "    selected_model_characteristics = model_characteristics[max_accuracy_index]\n",
    "\n",
    "    print(\"Accuracies for each model =\", max_depth_accuracies)\n",
    "\n",
    "    #plot for Information Gain Criteria\n",
    "    plt.plot(max_depths,max_depth_accuracies[0:5])\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Validation Accuracy (Information Gain Criteria)')\n",
    "    plt.show()\n",
    "\n",
    "    #plot for Gini Criteria\n",
    "    plt.plot(max_depths,max_depth_accuracies[5:10])\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Validation Accuracy (Gini Criteria)')\n",
    "    plt.show()\n",
    "\n",
    "    return selected_model_characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-J9BxriCf0V"
   },
   "source": [
    "Decision Tree Visualiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iho-eOmA3sQo"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "def visualizer(model, feature_names):\n",
    "    dot_format = tree.export_graphviz(model, feature_names = feature_names)\n",
    "    graph = graphviz.Source(dot_format)\n",
    "\n",
    "    return graph\n",
    "\n",
    "visualizer(Decision_Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgIS88JDCj16"
   },
   "source": [
    "Information Gain Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gpVWMjt4HMo"
   },
   "outputs": [],
   "source": [
    "def compute_information_gain(dataset, keyword_xi, feature_names):\n",
    "    X_training = dataset[0]; Y_training = dataset[1];\n",
    "\n",
    "    probability_real = Y_training.count(1)/len(Y_training) #Find probability of real in training dataset\n",
    "    h = entropy([probability_real, 1 - probability_real], base = 2) #Find entropy of the entire training dataset\n",
    "\n",
    "    absence_indicator = []\n",
    "    presence_indicator = []\n",
    "    keyword_index = feature_names.index(keyword_xi) #get position of keyword assigned during vectorization\n",
    "    keyword_presence_indicators = X_training[:,keyword_index] #get count of keywords in each line\n",
    "\n",
    "    for indicator in keyword_presence_indicators:\n",
    "            absence_indicator.append(indicator==0)  #collection of absence indication for all lines\n",
    "            presence_indicator.append(indicator!=0) #collection of presence indication for all lines\n",
    "\n",
    "    probability_absent = sum(absence_indicator)/len(absence_indicator)\n",
    "    probability_present = sum(presence_indicator)/len(presence_indicator)\n",
    "    if probability_absent == 0:\n",
    "        return 0\n",
    "    elif probability_absent == 1:\n",
    "        return 0\n",
    "\n",
    "    Y_training = numpy.array(Y_training)\n",
    "    prob_real_given_absent = Y_training[absence_indicator].sum()/len(Y_training[absence_indicator])\n",
    "    prob_real_given_present = Y_training[presence_indicator].sum()/len(Y_training[presence_indicator])\n",
    "    h_real_given_absent = entropy ([prob_real_given_absent, 1-prob_real_given_absent], base = 2)\n",
    "    h_real_given_present = entropy ([prob_real_given_present, 1-prob_real_given_absent], base = 2)\n",
    "\n",
    "    info_gain = h - probability_present*h_real_given_present - probability_absent*h_real_given_absent\n",
    "    return info_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72v3GxnWC0L4"
   },
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeRKAe4cC0XI"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  information_gained = []\n",
    "\n",
    "  max_depths = [1,5,20,50,100]\n",
    "\n",
    "  data = load_data('clean_real.txt','clean_fake.txt')\n",
    "\n",
    "  feature_names = data[6]\n",
    "\n",
    "  split_criteria = [\"entropy\", \"gini\"]\n",
    "\n",
    "  model = select_model(data, max_depths, split_criteria)[3]\n",
    "  diagram = visualizer(model, feature_names)\n",
    "\n",
    "  keywords = [\"the\",\"hillary\",\"donald\"]\n",
    "  for keyword in keywords:\n",
    "    information_gained.append(compute_information_gain(data,keyword,feature_names))\n",
    "\n",
    "  print(\"Information Gained = \", information_gained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvpzTEkTEQF_"
   },
   "outputs": [],
   "source": [
    "if __name__ = \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
